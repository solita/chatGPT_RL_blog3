{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import warnings\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Let's do a relative import\n",
    "import sys\n",
    "import os\n",
    "\n",
    "SCRIPT_DIR = os.path.dirname(os.path.abspath('../src_JokkeRuokolainen/*/'))\n",
    "sys.path.append(os.path.dirname(SCRIPT_DIR))\n",
    "\n",
    "from src_JokkeRuokolainen.environment_JokkeRuokolainen import CabDriver\n",
    "from src_JokkeRuokolainen.agent_JokkeRuokolainen import DuelingQAgent\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define taxi pickup time matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the time matrix provided\n",
    "Time_matrix = np.load(\"../Data/Inputs/TM.npy\")\n",
    "print(Time_matrix.shape)\n",
    "print(Time_matrix[3][4][17][5])\n",
    "# Example: (Returns Time Taken) 𝑇𝑖𝑚𝑒−𝑚𝑎𝑡𝑟𝑖𝑥[𝑠𝑡𝑎𝑟𝑡−𝑙𝑜𝑐][𝑒𝑛𝑑−𝑙𝑜𝑐][ℎ𝑜𝑢𝑟−𝑜𝑓−𝑡ℎ𝑒−𝑑𝑎𝑦] [𝑑𝑎𝑦−𝑜𝑓−𝑡ℎ𝑒−𝑤𝑒𝑒𝑘]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating tuples of action_index and actions from action_space\n",
    "env = CabDriver()\n",
    "cab_action_space = env.action_space\n",
    "cab_action_indices = [i for i in range(len(cab_action_space))]\n",
    "action_list = [\n",
    "    i for i in zip(cab_action_indices, cab_action_space)\n",
    "]  # tuples (action_indices, action)\n",
    "print(\"Action List (action_indices, action):\")\n",
    "action_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understanding the min and max time duration of trips from one point to another\n",
    "\n",
    "print(\"Minimimum time taken:\", Time_matrix.min())\n",
    "print(\"Maximum time taken:\", Time_matrix.max())\n",
    "print(\"Average time taken:\", Time_matrix.mean())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximum time taken is 11 hours. \n",
    "\n",
    "So, it is safe to say that the maximum time taken by the cab driver to move from one point to another is less than 1 day.\n",
    "\n",
    "### Initialize tracking of state-action pairs during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "states_track = collections.defaultdict(dict)\n",
    "\n",
    "\n",
    "def initialize_tracking_states():\n",
    "    sample_q_values = [\n",
    "        ((3, 0, 2), (3, 1)),\n",
    "        ((1, 6, 3), (2, 3)),\n",
    "        ((2, 2, 2), (3, 2)),\n",
    "        ((3, 10, 6), (3, 4)),\n",
    "        ((0, 20, 3), (1, 4)),\n",
    "        ((1, 23, 3), (1, 4)),\n",
    "    ]\n",
    "    for state, action in sample_q_values:\n",
    "        states_track[state][action] = []\n",
    "\n",
    "\n",
    "initialize_tracking_states()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Random State\n",
    "\n",
    "print(\"Random State Initialization:\")\n",
    "for i in range(5):  # Checking for 5 episodes\n",
    "    env = CabDriver()\n",
    "    random_state_init = env.state_init\n",
    "    print(random_state_init)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understanding action_size\n",
    "action_size = len(env.action_space)\n",
    "print(\"action_size:\", action_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tracking_states():\n",
    "    state_encod = np.array([agent.convert_state_to_vector(state)\n",
    "                           for state in states_track.keys()])\n",
    "    state_encod = np.reshape(state_encod, [-1, agent.state_size])\n",
    "    predictions = agent.model.predict(state_encod, verbose=0)\n",
    "\n",
    "    for (state, actions), prediction in zip(states_track.items(), predictions):\n",
    "        for action in actions:\n",
    "            action_index = env.action_space.index(list(action))\n",
    "            Q = prediction[action_index]\n",
    "            states_track[state][action].append(Q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Defining a function to save the Q-dictionary as a pickle file\n",
    "\n",
    "\n",
    "def save_obj(obj, name):\n",
    "    with open(name + \".pkl\", \"wb\") as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reset environment and initialize DDQN Agent class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_time = 24 * 30  # 24 hrs for 30 days per episode\n",
    "Episodes = 200  # No. of Episodes\n",
    "m = 5  # No. Locations\n",
    "t = 24  # No. of hrs in a day\n",
    "d = 7  # No. of days in a week\n",
    "# Reset environment\n",
    "action_space, state_space, state = env.reset()\n",
    "\n",
    "# Set up state and action sizes.\n",
    "state_size = m + t + d  # Network uses state as input\n",
    "action_size = len(action_space)\n",
    "\n",
    "# Invoke agent class\n",
    "agent = DuelingQAgent(action_size=action_size, state_size=state_size)\n",
    "\n",
    "# to store rewards in each episode\n",
    "rewards_per_episode, episodes = [], []\n",
    "# Rewards for state\n",
    "rewards_init_state = []\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "# tracking average reward per episode = total rewards in an episode/ total steps in an episode\n",
    "avg_reward = []\n",
    "# tracking total rewards per episode\n",
    "total_reward = []\n",
    "terminal_state = False\n",
    "env = CabDriver()\n",
    "for episode in range(0, Episodes):\n",
    "    # tracking total rewards, step count\n",
    "    tot_reward = 0\n",
    "    step_count = 0\n",
    "    total_time = 0  # Total time driver rode in this episode\n",
    "    terminal_state = False\n",
    "\n",
    "    # Reset at the start of each episode\n",
    "    action_space, state_space, state = env.reset()\n",
    "    # State initialization\n",
    "    initial_state = state\n",
    "\n",
    "    while not terminal_state:\n",
    "        # 1. Pick epsilon-greedy action from possible actions for the current state.\n",
    "        possible_actions_index, actions = env.requests(state)\n",
    "        action = agent.get_action(state, possible_actions_index)\n",
    "        if agent.epsilon > agent.epsilon_min:\n",
    "            agent.epsilon = agent.exponential_decay(agent.epsilon_max, agent.epsilon_decay, step_count)\n",
    "        # 2. Evaluate your reward and next state\n",
    "        next_state, reward, step_time = env.step(\n",
    "            state, env.action_space[action], Time_matrix\n",
    "        )\n",
    "        # 3. Total time driver rode in this episode\n",
    "        total_time += step_time\n",
    "        if total_time > episode_time:\n",
    "            # The cab driver accepts the last ride prior to the end of total time limit (720 hours).\n",
    "            # So the last trip begins before the end of 720 hrs but the total time of episode might cross 720 hrs.\n",
    "            terminal_state = True\n",
    "        else:\n",
    "            # 4. Append the experience to the memory\n",
    "            # calculate the temporal difference error\n",
    "            agent.append_sample(state, action, reward,\n",
    "                                next_state, terminal_state)\n",
    "            # Note: Here action is action index\n",
    "            # 5. Train the model by calling function agent.train_model\n",
    "            agent.train_model()\n",
    "            # 6. Update current state\n",
    "            state = next_state\n",
    "            tot_reward += reward\n",
    "            step_count += 1\n",
    "\n",
    "        # 7. Keep a track of rewards, Q-values, loss, etc.\n",
    "        # (Note: Loss were tracking is the model loss='mse')\n",
    "        if terminal_state and episode % 25 == 0:\n",
    "            avg_reward.append(tot_reward / step_count)\n",
    "            total_reward.append(tot_reward)\n",
    "            print(\n",
    "                \"episode:\",\n",
    "                episode,\n",
    "                \"  score:\",\n",
    "                tot_reward,\n",
    "                \"  memory length:\",\n",
    "                len(agent.memory),\n",
    "                \"  epsilon:\",\n",
    "                agent.epsilon,\n",
    "            )\n",
    "\n",
    "    # Store 'agent_model' every 200th episode\n",
    "    if episode % 200 == 0:\n",
    "        agent.save(f\"../Data/Outputs/chatGPT_dev/cab_driver.h5\")\n",
    "\n",
    "    # Every 25th episode\n",
    "    if episode % 25 == 0:\n",
    "        save_obj(avg_reward, \"../Data/Outputs/chatGPT_dev/Rewards\")\n",
    "        save_tracking_states()\n",
    "        save_obj(states_track, \"../Data/Outputs/chatGPT_dev/States_tracked\")\n",
    "    # Every 10000th episodes\n",
    "    if episode % 10000 == 0 and episode != 0:\n",
    "        plt.plot(list(range(len(avg_reward))), avg_reward)\n",
    "        plt.show()\n",
    "    # Saving the 'DQN_model' and 'model_weights' every 1000th episode.\n",
    "    if episode % 1000 == 0:\n",
    "        print(\"Saving Model {}\".format(episode))\n",
    "        # Saves DQN model in Keras H5 format\n",
    "        agent.save(name=\"../Data/Outputs/chatGPT_dev/DQN_model.h5\")\n",
    "        print(\"Saving Model {} Weights\".format(episode))\n",
    "        agent.save_weights_numpy(\n",
    "            name=\"../Data/Outputs/chatGPT_dev/model_weights.pkl\"\n",
    "        )  # Saves model_weights in pkl file\n",
    "        # (model_weights pickle file has a list of numpy arrays)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Elapsed time in sec: {elapsed_time}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot convergence and tracked states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting average rewards\n",
    "# x-values = 20000 episodes tracked after every 25th episode\n",
    "plt.plot(list(range(len(avg_reward))), avg_reward)\n",
    "plt.ylabel(\"Average Reward\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting total rewards\n",
    "plt.plot(list(range(len(total_reward))), total_reward)\n",
    "plt.ylabel(\"Total reward\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 7))\n",
    "plt.subplots_adjust(hspace=0.4)\n",
    "\n",
    "subplot_num = 241\n",
    "\n",
    "# loop over all the states in states_track dictionary\n",
    "for i, (state, actions) in enumerate(states_track.items()):\n",
    "    # loop over all the actions of each state\n",
    "    for j, (action, q_values) in enumerate(actions.items()):\n",
    "        xaxis = np.asarray(range(len(q_values)))\n",
    "        plt.subplot(subplot_num)\n",
    "        plt.plot(xaxis, np.asarray(q_values),label=f\"{state} {action}\")\n",
    "        plt.ylabel(\"Q-value\")\n",
    "        plt.title(f\"State: {state} \\n Action: {action}\")\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        subplot_num += 1\n",
    "\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.suptitle(\"Q-value over Steps for Different States and Actions\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_metal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 | packaged by conda-forge | (main, Nov 22 2022, 08:48:25) \n[Clang 14.0.6 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4cc2740515b041fd7abc7eb1e77c8b2d2a718c298b44c5e596e19940f3725ba7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
